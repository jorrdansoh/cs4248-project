{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97376846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Macro Score:  0.7639833906417784\n",
      "F1 Micro Score:  0.767\n",
      "[[567  92  41  50]\n",
      " [ 37 447 229  37]\n",
      " [ 33  19 603  95]\n",
      " [ 37   6  23 684]]\n",
      "Time taken: 705.8404417037964\n"
     ]
    }
   ],
   "source": [
    "# BEST MODEL\n",
    "\n",
    "# Feature Engineering: Stopwords and Oversampling and Stemming\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download(\"averaged_perceptron_tagger\")\n",
    "# nltk.download(\"maxent_ne_chunker\")\n",
    "# nltk.download(\"words\")\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = list(filter(lambda token: token not in STOPWORDS, tokens))\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    tokens = [ps.stem(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def features(X_train):\n",
    "    stop_words_features = []\n",
    "    for text in X_train:\n",
    "        words = word_tokenize(text.lower())\n",
    "        n = len(words)\n",
    "        if n == 0:\n",
    "            stop_words_features.append(0)\n",
    "            continue\n",
    "        num_stop_words = 0\n",
    "        num_exclamation_marks = 0\n",
    "        for word in words:\n",
    "            if word in STOPWORDS:\n",
    "                num_stop_words += 1\n",
    "\n",
    "        stop_words_features.append(num_stop_words/n * 100)\n",
    "    \n",
    "    stop_words_features = np.array(stop_words_features).reshape(-1, 1)\n",
    "    \n",
    "    result = np.hstack((\n",
    "        stop_words_features,\n",
    "        ))\n",
    "    return result\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "resampler = RandomOverSampler(random_state=42)\n",
    "model = ImbPipeline(steps=[\n",
    "        (\"features\", FeatureUnion([\n",
    "            (\"tfidf\", TfidfVectorizer(preprocessor=preprocess_text)),\n",
    "            (\"otherfeatures\", FunctionTransformer(features, validate=False)),\n",
    "        ], n_jobs=-1)),\n",
    "        (\"resampler\", resampler),\n",
    "        (\"classifier\", LogisticRegression(C=0.9, max_iter=10000))\n",
    "    ])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "f1score_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1score_micro = f1_score(y_test, y_pred, average='micro')\n",
    "print(\"F1 Macro Score: \", f1score_macro)\n",
    "print(\"F1 Micro Score: \", f1score_micro)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Time taken: {}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69a649c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Macro Score:  0.7428708029943922\n",
      "F1 Micro Score:  0.75\n",
      "[[593  67  41  49]\n",
      " [ 40 359 312  39]\n",
      " [ 34   9 599 108]\n",
      " [ 22   5  24 699]]\n",
      "Time taken: 92.65294551849365\n"
     ]
    }
   ],
   "source": [
    "# NO CROSS VALIDATION\n",
    "\n",
    "# Feature Engineering: Stopwords and Oversampling and Stemming\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download(\"averaged_perceptron_tagger\")\n",
    "# nltk.download(\"maxent_ne_chunker\")\n",
    "# nltk.download(\"words\")\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = list(filter(lambda token: token not in STOPWORDS, tokens))\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    tokens = [ps.stem(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def features(X_train):\n",
    "    stop_words_features = []\n",
    "    for text in X_train:\n",
    "        words = word_tokenize(text.lower())\n",
    "        n = len(words)\n",
    "        if n == 0:\n",
    "            stop_words_features.append(0)\n",
    "            continue\n",
    "        num_stop_words = 0\n",
    "        num_exclamation_marks = 0\n",
    "        for word in words:\n",
    "            if word in STOPWORDS:\n",
    "                num_stop_words += 1\n",
    "\n",
    "        stop_words_features.append(num_stop_words/n * 100)\n",
    "    \n",
    "    stop_words_features = np.array(stop_words_features).reshape(-1, 1)\n",
    "    \n",
    "    result = np.hstack((\n",
    "        stop_words_features,\n",
    "        ))\n",
    "    return result\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "resampler = RandomOverSampler(random_state=42)\n",
    "model = ImbPipeline(steps=[\n",
    "#         (\"features\", FeatureUnion([\n",
    "#             (\"tfidf\", TfidfVectorizer(preprocessor=preprocess_text)),\n",
    "#             (\"otherfeatures\", FunctionTransformer(features, validate=False)),\n",
    "#         ])),\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"resampler\", resampler),\n",
    "        (\"classifier\", LogisticRegression(max_iter=10000))\n",
    "    ])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "f1score_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1score_micro = f1_score(y_test, y_pred, average='micro')\n",
    "print(\"F1 Macro Score: \", f1score_macro)\n",
    "print(\"F1 Micro Score: \", f1score_micro)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Time taken: {}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01255fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[567  92  41  50]\n",
      " [ 37 447 229  37]\n",
      " [ 33  19 603  95]\n",
      " [ 37   6  23 684]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ea9bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "# Oversampling\n",
    "# Undersampling\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def main():\n",
    "    train = pd.read_csv('./raw_data/fulltrain.csv',  header=None)\n",
    "    y_train = train.iloc[:, 0]\n",
    "    X_train = train.iloc[:, 1]\n",
    "    test = pd.read_csv('./raw_data/balancedtest.csv', header=None)\n",
    "    y_test = test.iloc[:, 0]\n",
    "    X_test = test.iloc[:, 1]\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    model = Pipeline(steps=[\n",
    "        (\"tfidf\", TfidfVectorizer(preprocessor=preprocess_text)),\n",
    "        (\"scaler\", MaxAbsScaler()),\n",
    "        (\"classifier\", LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    f1score_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    f1score_micro = f1_score(y_test, y_pred, average='micro')\n",
    "    print(\"F1 Macro Score: \", f1score_macro)\n",
    "    print(\"F1 Micro Score: \", f1score_micro)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print('Time taken: {}'.format(time.time() - start))\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7406c44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Macro Score:  0.7048288887802756\n",
      "F1 Micro Score:  0.7195731910636879\n",
      "[[592  41  59  57]\n",
      " [ 36 260 409  45]\n",
      " [ 32  10 613  95]\n",
      " [ 22   3  32 693]]\n",
      "Time taken: 202.69247174263\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3373ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e196a01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    17870\n",
      "1    14047\n",
      "4     9995\n",
      "2     6942\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Read Training and Test values\n",
    "train = pd.read_csv('./raw_data/fulltrain.csv', header=None)\n",
    "y_train = train.iloc[:, 0]\n",
    "X_train = train.iloc[:, 1]\n",
    "test = pd.read_csv('./raw_data/balancedtest.csv', header=None)\n",
    "y_test = test.iloc[:, 0]\n",
    "X_test = test.iloc[:, 1]\n",
    "\n",
    "# Class Imbalance\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2baf8c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Macro Score:  0.7048891143800079\n",
      "F1 Micro Score:  0.7196666666666667\n",
      "[[593  41  59  57]\n",
      " [ 36 260 409  45]\n",
      " [ 32  10 613  95]\n",
      " [ 22   3  32 693]]\n",
      "Time taken: 209.89469838142395\n"
     ]
    }
   ],
   "source": [
    "# Base: TF-IDF, MaxAbsScaler, LR\n",
    "start = time.time()\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    (\"tfidf\", TfidfVectorizer(preprocessor=preprocess_text)),\n",
    "    (\"scaler\", MaxAbsScaler()),\n",
    "    (\"classifier\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "f1score_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1score_micro = f1_score(y_test, y_pred, average='micro')\n",
    "print(\"F1 Macro Score: \", f1score_macro)\n",
    "print(\"F1 Micro Score: \", f1score_micro)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Time taken: {}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4666c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomOverSampler\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "resampler = RandomOverSampler(random_state=42)\n",
    "model = ImbPipeline(steps=[\n",
    "        (\"tfidf\", TfidfVectorizer(preprocessor=preprocess_text)),\n",
    "        (\"resampler\", resampler),\n",
    "        (\"scaler\", MaxAbsScaler()),\n",
    "        (\"classifier\", LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "f1score_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1score_micro = f1_score(y_test, y_pred, average='micro')\n",
    "print(\"F1 Macro Score: \", f1score_macro)\n",
    "print(\"F1 Micro Score: \", f1score_micro)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Time taken: {}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f8a46a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Macro Score:  0.7165847774946114\n",
      "F1 Micro Score:  0.72757585861954\n",
      "[[583  51  56  59]\n",
      " [ 35 299 369  47]\n",
      " [ 31  10 600 109]\n",
      " [ 20   3  27 700]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'start' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-fb9378fe339b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"F1 Micro Score: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1score_micro\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Time taken: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'start' is not defined"
     ]
    }
   ],
   "source": [
    "# SMOTE\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "model = ImbPipeline(steps=[\n",
    "        (\"tfidf\", TfidfVectorizer(preprocessor=preprocess_text)),\n",
    "        (\"smote\", smote),\n",
    "        (\"scaler\", MaxAbsScaler()),\n",
    "        (\"classifier\", LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "f1score_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1score_micro = f1_score(y_test, y_pred, average='micro')\n",
    "print(\"F1 Macro Score: \", f1score_macro)\n",
    "print(\"F1 Micro Score: \", f1score_micro)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Time taken: {}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db18f7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Macro Score:  0.7281367881901537\n",
      "F1 Micro Score:  0.7349116372124042\n",
      "[[555  65  47  82]\n",
      " [ 32 356 310  52]\n",
      " [ 26  11 592 121]\n",
      " [ 17   5  27 701]]\n",
      "Time taken: 182.00884199142456\n"
     ]
    }
   ],
   "source": [
    "# RandomUnderSampler\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "resampler = RandomUnderSampler()\n",
    "model = ImbPipeline(steps=[\n",
    "        (\"tfidf\", TfidfVectorizer(preprocessor=preprocess_text)),\n",
    "        (\"resampler\", resampler),\n",
    "        (\"scaler\", MaxAbsScaler()),\n",
    "        (\"classifier\", LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "f1score_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1score_micro = f1_score(y_test, y_pred, average='micro')\n",
    "print(\"F1 Macro Score: \", f1score_macro)\n",
    "print(\"F1 Micro Score: \", f1score_micro)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Time taken: {}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36f2b8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Macro Score:  0.7118336109231469\n",
      "F1 Micro Score:  0.7259086362120707\n",
      "[[605  42  53  49]\n",
      " [ 38 268 402  42]\n",
      " [ 32  10 612  96]\n",
      " [ 22   5  31 692]]\n",
      "Time taken: 220.72338271141052\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering: Stopwords\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download(\"averaged_perceptron_tagger\")\n",
    "# nltk.download(\"maxent_ne_chunker\")\n",
    "# nltk.download(\"words\")\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "def features(X_train):\n",
    "    stop_words_features = []\n",
    "    for text in X_train:\n",
    "        words = word_tokenize(text.lower())\n",
    "        n = len(words)\n",
    "        if n == 0:\n",
    "            stop_words_features.append(0)\n",
    "            continue\n",
    "        num_stop_words = 0\n",
    "        for word in words:\n",
    "            if word in STOPWORDS:\n",
    "                num_stop_words += 1\n",
    "\n",
    "        stop_words_features.append(num_stop_words/n * 100)\n",
    "    \n",
    "    stop_words_features = np.array(stop_words_features).reshape(-1, 1)\n",
    "    \n",
    "    result = np.hstack((\n",
    "        stop_words_features,\n",
    "        ))\n",
    "    return result\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model = ImbPipeline(steps=[\n",
    "        (\"features\", FeatureUnion([\n",
    "            (\"tfidf\", TfidfVectorizer(preprocessor=preprocess_text)),\n",
    "            (\"otherfeatures\", FunctionTransformer(features, validate=False)),\n",
    "        ], n_jobs=-1)),\n",
    "        (\"scaler\", MaxAbsScaler()),\n",
    "        (\"classifier\", LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "f1score_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1score_micro = f1_score(y_test, y_pred, average='micro')\n",
    "print(\"F1 Macro Score: \", f1score_macro)\n",
    "print(\"F1 Micro Score: \", f1score_micro)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Time taken: {}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f23ca4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Macro Score:  0.7090237293356257\n",
      "F1 Micro Score:  0.7239079693231076\n",
      "[[595  44  55  55]\n",
      " [ 37 262 408  43]\n",
      " [ 31  10 619  90]\n",
      " [ 22   3  30 695]]\n",
      "Time taken: 226.51388120651245\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering: Punctuations\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download(\"averaged_perceptron_tagger\")\n",
    "# nltk.download(\"maxent_ne_chunker\")\n",
    "# nltk.download(\"words\")\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "PUNCTUATIONS = set(string.punctuation)\n",
    "NEGATIVE_WORDS = set([\"n't\", \"not\", \"no\"])\n",
    "\n",
    "def features(X_train):\n",
    "    stop_words_features = []\n",
    "    punctuations_features = []\n",
    "    negative_words_features = []\n",
    "    for text in X_train:\n",
    "        words = word_tokenize(text.lower())\n",
    "        n = len(words)\n",
    "        if n == 0:\n",
    "            stop_words_features.append(0)\n",
    "            continue\n",
    "        num_stop_words = 0\n",
    "        num_punctuations = 0\n",
    "        for char in text:\n",
    "            if char in STOPWORDS:\n",
    "                num_stop_words += 1\n",
    "            if char in PUNCTUATIONS:\n",
    "                num_punctuations += 1\n",
    "        \n",
    "        num_negative_words = 0\n",
    "        for word in words:\n",
    "            if word in NEGATIVE_WORDS:\n",
    "                num_negative_words += 1\n",
    "\n",
    "        stop_words_features.append(num_stop_words/n * 100)\n",
    "        punctuations_features.append(num_punctuations/n * 100)\n",
    "        negative_words_features.append(num_negative_words/n * 100)\n",
    "    \n",
    "    stop_words_features = np.array(stop_words_features).reshape(-1, 1)\n",
    "    punctuations_features = np.array(punctuations_features).reshape(-1, 1)\n",
    "    negative_words_features = np.array(negative_words_features).reshape(-1, 1)\n",
    "    \n",
    "    result = np.hstack((\n",
    "#         stop_words_features, \n",
    "        punctuations_features, \n",
    "#         negative_words_features,\n",
    "        ))\n",
    "    return result\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model = ImbPipeline(steps=[\n",
    "        (\"features\", FeatureUnion([\n",
    "            (\"tfidf\", TfidfVectorizer(preprocessor=preprocess_text)),\n",
    "            (\"otherfeatures\", FunctionTransformer(features, validate=False)),\n",
    "        ], n_jobs=-1)),\n",
    "        (\"scaler\", MaxAbsScaler()),\n",
    "        (\"classifier\", LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "f1score_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1score_micro = f1_score(y_test, y_pred, average='micro')\n",
    "print(\"F1 Macro Score: \", f1score_macro)\n",
    "print(\"F1 Micro Score: \", f1score_micro)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Time taken: {}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4be211d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Macro Score:  0.7219595634326925\n",
      "F1 Micro Score:  0.732\n",
      "[[588  55  51  56]\n",
      " [ 33 311 361  45]\n",
      " [ 32  10 596 112]\n",
      " [ 20   5  24 701]]\n",
      "Time taken: 211.09539985656738\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering: Stopwords and Oversampling\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download(\"averaged_perceptron_tagger\")\n",
    "# nltk.download(\"maxent_ne_chunker\")\n",
    "# nltk.download(\"words\")\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "def features(X_train):\n",
    "    stop_words_features = []\n",
    "    for text in X_train:\n",
    "        words = word_tokenize(text.lower())\n",
    "        n = len(words)\n",
    "        if n == 0:\n",
    "            stop_words_features.append(0)\n",
    "            continue\n",
    "        num_stop_words = 0\n",
    "        for word in words:\n",
    "            if word in STOPWORDS:\n",
    "                num_stop_words += 1\n",
    "\n",
    "        stop_words_features.append(num_stop_words/n * 100)\n",
    "    \n",
    "    stop_words_features = np.array(stop_words_features).reshape(-1, 1)\n",
    "    \n",
    "    result = np.hstack((\n",
    "        stop_words_features,\n",
    "        ))\n",
    "    return result\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "resampler = RandomOverSampler(random_state=42)\n",
    "model = ImbPipeline(steps=[\n",
    "        (\"features\", FeatureUnion([\n",
    "            (\"tfidf\", TfidfVectorizer(preprocessor=preprocess_text)),\n",
    "            (\"otherfeatures\", FunctionTransformer(features, validate=False)),\n",
    "        ], n_jobs=-1)),\n",
    "        (\"resampler\", resampler),\n",
    "        (\"scaler\", MaxAbsScaler()),\n",
    "        (\"classifier\", LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "f1score_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1score_micro = f1_score(y_test, y_pred, average='micro')\n",
    "print(\"F1 Macro Score: \", f1score_macro)\n",
    "print(\"F1 Micro Score: \", f1score_micro)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Time taken: {}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6fc31a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Macro Score:  0.7211237548348766\n",
      "F1 Micro Score:  0.7313333333333333\n",
      "[[578  56  50  66]\n",
      " [ 33 313 357  47]\n",
      " [ 31  11 600 108]\n",
      " [ 22   3  22 703]]\n",
      "Time taken: 206.73965406417847\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering: Stopwords and Oversampling and Punctuations\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download(\"averaged_perceptron_tagger\")\n",
    "# nltk.download(\"maxent_ne_chunker\")\n",
    "# nltk.download(\"words\")\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "PUNCTUATIONS = set(string.punctuation)\n",
    "\n",
    "def features(X_train):\n",
    "    stop_words_features = []\n",
    "    punctuations_features = []\n",
    "    \n",
    "    for text in X_train:\n",
    "        words = word_tokenize(text.lower())\n",
    "        n = len(words)\n",
    "        if n == 0:\n",
    "            stop_words_features.append(0)\n",
    "            continue\n",
    "        num_stop_words = 0\n",
    "        for word in words:\n",
    "            if word in STOPWORDS:\n",
    "                num_stop_words += 1\n",
    "        num_punctuations = 0\n",
    "        for char in text:\n",
    "            if char in STOPWORDS:\n",
    "                num_stop_words += 1\n",
    "            if char in PUNCTUATIONS:\n",
    "                num_punctuations += 1\n",
    "                \n",
    "        stop_words_features.append(num_stop_words/n * 100)\n",
    "        punctuations_features.append(num_punctuations/n * 100)\n",
    "    \n",
    "    stop_words_features = np.array(stop_words_features).reshape(-1, 1)\n",
    "    punctuations_features = np.array(punctuations_features).reshape(-1, 1)\n",
    "    \n",
    "    result = np.hstack((\n",
    "        stop_words_features,\n",
    "        punctuations_features\n",
    "        ))\n",
    "    return result\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "resampler = RandomOverSampler(random_state=42)\n",
    "model = ImbPipeline(steps=[\n",
    "        (\"features\", FeatureUnion([\n",
    "            (\"tfidf\", TfidfVectorizer(preprocessor=preprocess_text)),\n",
    "            (\"otherfeatures\", FunctionTransformer(features, validate=False)),\n",
    "        ], n_jobs=-1)),\n",
    "        (\"resampler\", resampler),\n",
    "        (\"scaler\", MaxAbsScaler()),\n",
    "        (\"classifier\", LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "f1score_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1score_micro = f1_score(y_test, y_pred, average='micro')\n",
    "print(\"F1 Macro Score: \", f1score_macro)\n",
    "print(\"F1 Micro Score: \", f1score_micro)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Time taken: {}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a24f764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./raw_data/train.csv')\n",
    "y_train = train.iloc[:, 0]\n",
    "X_train = train.iloc[:, 1]\n",
    "val = pd.read_csv('./raw_data/val.csv')\n",
    "y_val = val.iloc[:, 0]\n",
    "X_val = val.iloc[:, 1]\n",
    "test = pd.read_csv('./raw_data/balancedtest.csv', header=None)\n",
    "y_test = test.iloc[:, 0]\n",
    "X_test = test.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69587989",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import f1_score, confusion_matrix, make_scorer\n",
    "\n",
    "def features(X_train):\n",
    "    stop_words_features = []\n",
    "    for text in X_train:\n",
    "        words = word_tokenize(text.lower())\n",
    "        n = len(words)\n",
    "        if n == 0:\n",
    "            stop_words_features.append(0)\n",
    "            continue\n",
    "        num_stop_words = 0\n",
    "        for word in words:\n",
    "            if word in STOPWORDS:\n",
    "                num_stop_words += 1\n",
    "\n",
    "        stop_words_features.append(num_stop_words/n * 100)\n",
    "    \n",
    "    stop_words_features = np.array(stop_words_features).reshape(-1, 1)\n",
    "    \n",
    "    result = np.hstack((\n",
    "        stop_words_features,\n",
    "        ))\n",
    "    return result\n",
    "\n",
    "parameters = {\n",
    "    'classifier__C': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "resampler = RandomOverSampler(random_state=42)\n",
    "model = ImbPipeline(steps=[\n",
    "        (\"features\", FeatureUnion([\n",
    "            (\"tfidf\", TfidfVectorizer(preprocessor=preprocess_text)),\n",
    "            (\"otherfeatures\", FunctionTransformer(features, validate=False)),\n",
    "        ], n_jobs=-1)),\n",
    "        (\"resampler\", resampler),\n",
    "        (\"scaler\", MaxAbsScaler()),\n",
    "        (\"classifier\", LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "\n",
    "\n",
    "scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "grid_search = GridSearchCV(model, parameters, cv=KFold(5), scoring=scorer, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(grid_search.best_params_)\n",
    "print()\n",
    "\n",
    "y_val_pred = grid_search.predict(X_val)\n",
    "print(\"Validation F1 Macro Score: \", f1_score(y_val, y_val_pred, average='macro'))\n",
    "print(\"Validation F1 Micro Score: \", f1_score(y_val, y_val_pred, average='micro'))\n",
    "\n",
    "y_test_pred = grid_search.predict(X_test)\n",
    "print(\"Test F1 Macro Score: \", f1_score(y_test, y_test_pred, average='macro'))\n",
    "print(\"Test F1 Micro Score: \", f1_score(y_test, y_test_pred, average='micro'))\n",
    "\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print('Time taken: {}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cf3c530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Macro Score:  0.7299696021831981\n",
      "F1 Micro Score:  0.739\n",
      "[[589  53  51  57]\n",
      " [ 28 321 354  47]\n",
      " [ 30  11 607 102]\n",
      " [ 17   5  28 700]]\n",
      "Time taken: 319.47268891334534\n"
     ]
    }
   ],
   "source": [
    "# NO CROSS VALIDATION\n",
    "\n",
    "# Feature Engineering: Stopwords and Oversampling and Lemmatization\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download(\"averaged_perceptron_tagger\")\n",
    "# nltk.download(\"maxent_ne_chunker\")\n",
    "# nltk.download(\"words\")\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def features(X_train):\n",
    "    stop_words_features = []\n",
    "    for text in X_train:\n",
    "        words = word_tokenize(text.lower())\n",
    "        n = len(words)\n",
    "        if n == 0:\n",
    "            stop_words_features.append(0)\n",
    "            continue\n",
    "        num_stop_words = 0\n",
    "        num_exclamation_marks = 0\n",
    "        for word in words:\n",
    "            if word in STOPWORDS:\n",
    "                num_stop_words += 1\n",
    "\n",
    "        stop_words_features.append(num_stop_words/n * 100)\n",
    "    \n",
    "    stop_words_features = np.array(stop_words_features).reshape(-1, 1)\n",
    "    \n",
    "    result = np.hstack((\n",
    "        stop_words_features,\n",
    "        ))\n",
    "    return result\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "resampler = RandomOverSampler(random_state=42)\n",
    "model = ImbPipeline(steps=[\n",
    "        (\"features\", FeatureUnion([\n",
    "            (\"tfidf\", TfidfVectorizer(preprocessor=preprocess_text)),\n",
    "            (\"otherfeatures\", FunctionTransformer(features, validate=False)),\n",
    "        ], n_jobs=-1)),\n",
    "        (\"resampler\", resampler),\n",
    "        (\"scaler\", MaxAbsScaler()),\n",
    "        (\"classifier\", LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "f1score_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1score_micro = f1_score(y_test, y_pred, average='micro')\n",
    "print(\"F1 Macro Score: \", f1score_macro)\n",
    "print(\"F1 Micro Score: \", f1score_micro)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Time taken: {}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00da9456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Macro Score:  0.7311154536427189\n",
      "F1 Micro Score:  0.74\n",
      "[[588  59  44  59]\n",
      " [ 33 330 339  48]\n",
      " [ 31  11 607 101]\n",
      " [ 25   3  27 695]]\n",
      "Time taken: 650.992427110672\n"
     ]
    }
   ],
   "source": [
    "# NO CROSS VALIDATION\n",
    "\n",
    "# Feature Engineering: Stopwords and Oversampling and Stemming and Lemmatization\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download(\"averaged_perceptron_tagger\")\n",
    "# nltk.download(\"maxent_ne_chunker\")\n",
    "# nltk.download(\"words\")\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    tokens = [ps.stem(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def features(X_train):\n",
    "    stop_words_features = []\n",
    "    for text in X_train:\n",
    "        words = word_tokenize(text.lower())\n",
    "        n = len(words)\n",
    "        if n == 0:\n",
    "            stop_words_features.append(0)\n",
    "            continue\n",
    "        num_stop_words = 0\n",
    "        num_exclamation_marks = 0\n",
    "        for word in words:\n",
    "            if word in STOPWORDS:\n",
    "                num_stop_words += 1\n",
    "\n",
    "        stop_words_features.append(num_stop_words/n * 100)\n",
    "    \n",
    "    stop_words_features = np.array(stop_words_features).reshape(-1, 1)\n",
    "    \n",
    "    result = np.hstack((\n",
    "        stop_words_features,\n",
    "        ))\n",
    "    return result\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "resampler = RandomOverSampler(random_state=42)\n",
    "model = ImbPipeline(steps=[\n",
    "        (\"features\", FeatureUnion([\n",
    "            (\"tfidf\", TfidfVectorizer(preprocessor=preprocess_text)),\n",
    "            (\"otherfeatures\", FunctionTransformer(features, validate=False)),\n",
    "        ], n_jobs=-1)),\n",
    "        (\"resampler\", resampler),\n",
    "        (\"scaler\", MaxAbsScaler()),\n",
    "        (\"classifier\", LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "f1score_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1score_micro = f1_score(y_test, y_pred, average='micro')\n",
    "print(\"F1 Macro Score: \", f1score_macro)\n",
    "print(\"F1 Micro Score: \", f1score_micro)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Time taken: {}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38bf9c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Macro Score:  0.7579663304080297\n",
      "F1 Micro Score:  0.7633333333333333\n",
      "[[599  71  36  44]\n",
      " [ 37 392 273  48]\n",
      " [ 33  13 610  94]\n",
      " [ 27   5  29 689]]\n",
      "Time taken: 821.6594815254211\n"
     ]
    }
   ],
   "source": [
    "# NO CROSS VALIDATION\n",
    "\n",
    "# Feature Engineering: Stopwords and Oversampling and Stemming\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download(\"averaged_perceptron_tagger\")\n",
    "# nltk.download(\"maxent_ne_chunker\")\n",
    "# nltk.download(\"words\")\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    tokens = [ps.stem(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def features(X_train):\n",
    "    stop_words_features = []\n",
    "    for text in X_train:\n",
    "        words = word_tokenize(text.lower())\n",
    "        n = len(words)\n",
    "        if n == 0:\n",
    "            stop_words_features.append(0)\n",
    "            continue\n",
    "        num_stop_words = 0\n",
    "        num_exclamation_marks = 0\n",
    "        for word in words:\n",
    "            if word in STOPWORDS:\n",
    "                num_stop_words += 1\n",
    "\n",
    "        stop_words_features.append(num_stop_words/n * 100)\n",
    "    \n",
    "    stop_words_features = np.array(stop_words_features).reshape(-1, 1)\n",
    "    \n",
    "    result = np.hstack((\n",
    "        stop_words_features,\n",
    "        ))\n",
    "    return result\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "resampler = RandomOverSampler(random_state=42)\n",
    "model = ImbPipeline(steps=[\n",
    "        (\"features\", FeatureUnion([\n",
    "            (\"tfidf\", TfidfVectorizer(preprocessor=preprocess_text)),\n",
    "            (\"otherfeatures\", FunctionTransformer(features, validate=False)),\n",
    "        ], n_jobs=-1)),\n",
    "        (\"resampler\", resampler),\n",
    "        (\"classifier\", LogisticRegression(C=0.9, max_iter=10000))\n",
    "    ])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "f1score_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1score_micro = f1_score(y_test, y_pred, average='micro')\n",
    "print(\"F1 Macro Score: \", f1score_macro)\n",
    "print(\"F1 Micro Score: \", f1score_micro)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Time taken: {}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbcdc0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Macro Score:  0.7274773193489167\n",
      "F1 Micro Score:  0.734\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0c8f4cf2d7d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"F1 Macro Score: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1score_macro\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"F1 Micro Score: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1score_micro\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Time taken: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# NO CROSS VALIDATION\n",
    "\n",
    "# Feature Engineering: Stopwords and Oversampling and Stemming and Stopwords Removal\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download(\"averaged_perceptron_tagger\")\n",
    "# nltk.download(\"maxent_ne_chunker\")\n",
    "# nltk.download(\"words\")\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = list(filter(lambda token: token not in STOPWORDS, tokens))\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    tokens = [ps.stem(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def features(X_train):\n",
    "    stop_words_features = []\n",
    "    for text in X_train:\n",
    "        words = word_tokenize(text.lower())\n",
    "        n = len(words)\n",
    "        if n == 0:\n",
    "            stop_words_features.append(0)\n",
    "            continue\n",
    "        num_stop_words = 0\n",
    "        num_exclamation_marks = 0\n",
    "        for word in words:\n",
    "            if word in STOPWORDS:\n",
    "                num_stop_words += 1\n",
    "\n",
    "        stop_words_features.append(num_stop_words/n * 100)\n",
    "    \n",
    "    stop_words_features = np.array(stop_words_features).reshape(-1, 1)\n",
    "    \n",
    "    result = np.hstack((\n",
    "        stop_words_features,\n",
    "        ))\n",
    "    return result\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "resampler = RandomOverSampler(random_state=42)\n",
    "model = ImbPipeline(steps=[\n",
    "        (\"features\", FeatureUnion([\n",
    "            (\"tfidf\", TfidfVectorizer(preprocessor=preprocess_text)),\n",
    "            (\"otherfeatures\", FunctionTransformer(features, validate=False)),\n",
    "        ], n_jobs=-1)),\n",
    "        (\"resampler\", resampler),\n",
    "        (\"scaler\", MaxAbsScaler()),\n",
    "        (\"classifier\", LogisticRegression(max_iter=10000))\n",
    "    ])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "f1score_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1score_micro = f1_score(y_test, y_pred, average='micro')\n",
    "print(\"F1 Macro Score: \", f1score_macro)\n",
    "print(\"F1 Micro Score: \", f1score_micro)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Time taken: {}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f310c361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Macro Score:  0.6665098613955905\n",
      "F1 Micro Score:  0.6803333333333333\n",
      "[[470 107 114  59]\n",
      " [ 87 268 371  24]\n",
      " [ 26   2 699  23]\n",
      " [ 88   4  54 604]]\n",
      "Time taken: 471.1072187423706\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "# NO CROSS VALIDATION\n",
    "\n",
    "# Feature Engineering: Stopwords and Oversampling and Stemming\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download(\"averaged_perceptron_tagger\")\n",
    "# nltk.download(\"maxent_ne_chunker\")\n",
    "# nltk.download(\"words\")\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    tokens = [ps.stem(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def features(X_train):\n",
    "    stop_words_features = []\n",
    "    for text in X_train:\n",
    "        words = word_tokenize(text.lower())\n",
    "        n = len(words)\n",
    "        if n == 0:\n",
    "            stop_words_features.append(0)\n",
    "            continue\n",
    "        num_stop_words = 0\n",
    "        num_exclamation_marks = 0\n",
    "        for word in words:\n",
    "            if word in STOPWORDS:\n",
    "                num_stop_words += 1\n",
    "\n",
    "        stop_words_features.append(num_stop_words/n * 100)\n",
    "    \n",
    "    stop_words_features = np.array(stop_words_features).reshape(-1, 1)\n",
    "    \n",
    "    result = np.hstack((\n",
    "        stop_words_features,\n",
    "        ))\n",
    "    return result\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "resampler = RandomOverSampler(random_state=42)\n",
    "model = Pipeline(steps=[\n",
    "        (\"countVectorizer\", CountVectorizer(preprocessor=preprocess_text)),\n",
    "        (\"classifier\", MultinomialNB())\n",
    "    ])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "f1score_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1score_micro = f1_score(y_test, y_pred, average='micro')\n",
    "print(\"F1 Macro Score: \", f1score_macro)\n",
    "print(\"F1 Micro Score: \", f1score_micro)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Time taken: {}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c726e77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Macro Score:  0.31537316615694866\n",
      "F1 Micro Score:  0.389\n",
      "[[267   2 481   0]\n",
      " [ 47   3 700   0]\n",
      " [  1   0 749   0]\n",
      " [122   0 480 148]]\n",
      "Time taken: 474.5859832763672\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "# NO CROSS VALIDATION\n",
    "\n",
    "# Feature Engineering: Stopwords and Oversampling and Stemming\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download(\"averaged_perceptron_tagger\")\n",
    "# nltk.download(\"maxent_ne_chunker\")\n",
    "# nltk.download(\"words\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    tokens = [ps.stem(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "resampler = RandomOverSampler(random_state=42)\n",
    "model = Pipeline(steps=[\n",
    "        (\"tfIdfVectorizer\", TfidfVectorizer(preprocessor=preprocess_text)),\n",
    "        (\"classifier\", MultinomialNB())\n",
    "    ])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "f1score_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1score_micro = f1_score(y_test, y_pred, average='micro')\n",
    "print(\"F1 Macro Score: \", f1score_macro)\n",
    "print(\"F1 Micro Score: \", f1score_micro)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Time taken: {}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3956159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
